@inproceedings{chr2020:Miller,
  author =       {Ben Miller and Julie Park},
  title =        {Computing Narrative},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short50.pdf},
  abstract =     {Presuming it is possible to produce an analytic model of narrative
                  representable in the architecture of computation, other affordances of
                  digital media should be adaptable to empirical, quantitative inquiry of
                  stories and their structure. Computing Narrative is a brief description
                  of some projects attempting those representations and the implications
                  thereof for critical inquiry of stories and their structure.}
}

@inproceedings{chr2020:Todorov,
  author =       {Konstantin Todorov and Giovanni Colavizza},
  title =        {Transfer Learning for Historical Corpora: An Assessment on Post-OCR
                  Correction and Named Entity Recognition},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long32.pdf},
  abstract =     {Transfer learning in Natural Language Processing, mainly in the form of
                  pre-trained language models, has recently delivered substantial gains
                  across a range of tasks. Scholars and practitioners working with OCRed
                  historical corpora are thus increasingly exploring the use of
                  pre-trained language models. Nevertheless, the specific challenges posed
                  by historical documents, including OCR quality and linguistic change,
                  call for a critical assessment of the use of pre-trained language models
                  in this setting. We consider two shared tasks, ICDAR2019 (post-OCR
                  correction) and CLEF-HIPE-2020 (Named Entity Recognition, NER), and
                  systematically assess using pre-trained language models with data in
                  French, German and English. We find that using pre-trained language
                  models helps with NER but less so with post-OCR correction. Pre-trained
                  language models should therefore be used critically when working with
                  OCRed historical corpora. We release our code base, in order to allow
                  replicating our results and testing other pre-trained representations.}
}

@inproceedings{chr2020:Sela,
  author =       {Artjoms Šeļa and Boris Orekhov and Roman Leibov},
  title =        {Weak Genres: Modeling Association Between Poetic Meter and Meaning in
                  Russian Poetry},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long35.pdf},
  abstract =     {This paper aims to formalize an established theory in versification
                  studies known as "semantic halo of a meter" which states that different
                  metrical forms in modern poetry accumulate and retain distinct semantic
                  associations. We use LDA topic modeling on a large-scale corpus of
                  Russian poetry (1750-1950) to represent each poem in one topic space and
                  then proceed to represent each meter as a distribution of aggregated
                  topic probabilities. Using unsupervised classification and extensive
                  sampling we show that robust form-meaning associations are present both
                  within and between metrical forms: two samples of the same meter tend to
                  appear most similar, while two metrical forms of the same family tend to
                  group together. This effect is present if corpus is controlled for
                  chronology and is not an artifact of population size. We argue that
                  similar approach could be used to align and compare semantic halos
                  across languages and traditions to give meaningful general-level answers
                  to questions of literary history.}
}

@inproceedings{chr2020:Smits,
  author =       {Thomas Smits and Ruben Ros},
  title =        {Quantifying Iconicity in 940K Online Circulations of 26 Iconic
                  Photographs},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short34.pdf},
  abstract =     {What impact do digital media have on the creation, selection,
                  distribution, reception and meaning of iconic photographs? Recent
                  studies have suggested that digital circulation, especially in a
                  memeified form, might lead to an ‘erosion,’ ‘fracturing,’ or
                  ‘collapsing’ of the original context and meaning of iconic pictures.
                  Using a close reading methodology, these studies are necessarily based
                  on a limited sample – in number, period and geographic distribution – of
                  online circulations. Introducing a distant reading methodology to the
                  study of iconic photographs, this paper applies the Google Cloud Vision
                  API to retrieve 940K online circulations of 26 iconic images between
                  1995 and 2020. We operationalize the ‘loss of meaning/context’
                  hypothesis by using document embeddings to study the relationship
                  between the iconic photographs and the text surrounding them on the
                  webpage. Based on this distant reading, we argue that the digital
                  circulation of iconic photographs is comprised of similar contextual,
                  self-referential and non-referential combinations of images and texts.}
}

@inproceedings{chr2020:Jannidis,
  author =       {Fotis Jannidis and Leonard Konle},
  title =        {Domain and Task Adaptive Pretraining for Language Models},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short33.pdf},
  abstract =     {All current state-of-the-art systems in NLP utilize transformer based
                  language models trained on massive amounts of text. This paper discusses
                  strategies to adapt these models to historical domains and tasks,
                  typical for research in the Computational Humanities. Using two
                  task-specific corpora from the same domain (literary texts from the 19th
                  Century) and Bert devlin2018bert resp. Distilbert sanh2019distilbert as
                  baselines, we can confirm results from a recent study that continuing
                  pretraining on the domain and the task data substantially improves task
                  performance. Training a model from scratch using Electra
                  clark2020electra is not competitive for our data sets.}
}

@inproceedings{chr2020:Liebl,
  author =       {Bernhard Liebl and Manuel Burghardt},
  title =        {From Historical Newspapers to Machine-Readable Data: The Origami OCR
                  Pipeline},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long20.pdf},
  abstract =     {While historical newspapers recently have gained a lot of attention in
                  the digital humanities, transforming them into machine-readable data by
                  means of OCR poses some major challenges. In order to address these
                  challenges, we have developed an end-to-end OCR pipeline named Origami .
                  This pipeline is part of a current project on the digitization and
                  quantitative analysis of the German newspaper Berliner Börsen-Zeitung
                  (BBZ), from 1872 to 1931. The Origami pipeline reuses existing open
                  source OCR components and on top offers a new configurable architecture
                  for layout detection, a simple table recognition, a two-stage X-Y cut
                  for reading order detection, and a new robust implementation for
                  document dewarping. In this paper we describe the different stages of
                  the workflow and discuss how they meet the above-mentioned challenges
                  posed by historical newspapers.}
}

@inproceedings{chr2020:Sharma,
  author =       {Aniruddha Sharma and Yuerong Hu and Peizhen Wu and Wenyi Shang and
                  Shubhangi Singhal and Ted Underwood},
  title =        {The Rise and Fall of Genre Differentiation in English-Language Fiction},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long27.pdf},
  abstract =     {The organization of fiction into genres is a relatively recent
                  innovation. We cast new light on the history of this practice by
                  studying the strength of the textual differentiation between genres, and
                  between genre fiction collectively and the rest of the fiction market,
                  in a collection of English-language books stretching from 1860 to 2009.
                  To measure differentiation, we adapt distance measures that have been
                  used to evaluate the strength of clustering. We use genre labels from
                  two different sources: the Library of Congress headings assigned by
                  librarians, and the genre categories implicit in book reviews published
                  by Kirkus Reviews , covering books from 1928 to 2009. Both sources
                  support an account that has genre differentiation rising to (roughly)
                  the middle of the twentieth century, and declining by the end of the
                  century.}
}

@inproceedings{chr2020:Gius,
  author =       {Evelyn Gius and Inna Uglanova},
  title =        {The Order of Things. A Study on Topic Modelling of Literary Texts},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long7.pdf},
  abstract =     {Topic modelling is considered a statistical tool for the thematic
                  decomposition of texts. In reality, it captures only statistical
                  patterns in the structure of the object. This sensitivity of the method
                  for structure makes it less effective when applied to literary texts in
                  which structure itself is a relevant feature with an artistic function.
                  In this paper, we calculate a series of topic models for three corpora
                  of literary narratives with various stages of data cleaning. We apply
                  coherence values, qualitative interpretation and measurement of topic
                  distances in order to shed some light on the regularities between text
                  features and the quality of the topic modelling performed for literary
                  prose.}
}

@inproceedings{chr2020:Lam,
  author =       {Charles Lam and Brian Leung and Cora Yip and Jason Yung},
  title =        {A Linguistic Approach to Misinformation in Chinese},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short29.pdf},
  abstract =     {Identifying useful information is increasingly important and difficult.
                  Correct information is crucial in when we make our decisions, regardless
                  in finance/economy, health and politics. Yet, the amount of
                  misinformation has been rising in all these aspects. Existing works
                  primarily focus on the truthfulness of information using data in
                  English, and either ignore unverifiable claims or categorize them with
                  misinformation (also known as `fake news'). However, this approach often
                  disregards misleading information or conspiracy, which can be as
                  dangerous as verifiably wrong information. From a linguistic
                  perspective, the present study analyzes headlines of 69170 extracted
                  articles in Chinese and identifies their linguistic features. Results
                  show that misinformation in Chinese use emotive language and hyperbole
                  to get readers' attention, which echoes previous studies on clickbaits
                  and shows that these tactics in misinformation are shared across
                  languages. We further argue that these tactics are particularly obvious,
                  when the articles are categorized based on the topics. Through an
                  analysis of commonly used phrases and keywords, we discuss how the word
                  list can be further developed into an identification system for
                  misinformation.}
}

@inproceedings{chr2020:Piotrowski,
  author =       {Michael Piotrowski and Mateusz Fafinski},
  title =        {Nothing New Under the Sun? Computational Humanities and the Methodology
                  of History},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short16.pdf},
  abstract =     {The example of historiography shows that quantitative methods have
                  already been part of the humanities for a long time. Such methods alone
                  therefore cannot be constitutive of the computational humanities (CH).
                  It is also problematic and unsustainable to conceive it as a kind of
                  ``toolbox'' of quantitative methods, as it places CH outside of the
                  methodological traditions of the humanities disciplines. Instead, we
                  need to remember that disciplines are defined by their research objects
                  and the research questions they tackle. This means that we need to
                  distinguish between applied and theoretical CH, and that applied CH must
                  be firmly placed in the methodological scope and tradition of their
                  mother disciplines. We posit that the supposed dichotomy of qualitative
                  and quantitative methods is fallacious: neither will quantitative
                  methods replace qualitative approaches in history, nor are they
                  unnecessary---they are complementary.}
}

@inproceedings{chr2020:Manjavacas,
  author =       {Enrique Manjavacas and Folgert Karsdorp and Mike Kestemont},
  title =        {A Statistical Foray into Contextual Aspects of Intertextuality},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long28.pdf},
  abstract =     {Intertextuality is a highly productive concept in literary theory. The
                  pervasiveness of intertextuality in literary texts has lead
                  simultaneously to a proliferation of applications with often divergent
                  interpretations of the concept of intertextuality, as well as a
                  recurrent interest in studying it from a computational point of view.
                  Despite the potential of data-driven, bottom-up approaches, most
                  computational research into intertextuality has focused on the matter of
                  text reuse detection, exploiting surface-level properties to improve the
                  performance of retrieval systems. In the present study, we utilize the
                  Patrologia Latina -- a substantial collection of religious texts
                  spanning over a millennium of Latin writing (3rd to 13th centuries) --
                  to provide a large-scale systematic study of biblical intertexts. On the
                  basis of multi-level statistical models, we investigate two axes of
                  intertexts: the degree of lexical similarity, and the degree to which
                  intertexts are thematically embedded in the context. Furthermore, we
                  investigate the extent to which the following contextual sources of
                  variation help explain the distribution of intertexts along the
                  aforementioned axes: first, we analyze the effect of authorship: do
                  authors differ in the way they compose their intertexts? Secondly, we
                  inspect factors related to the source collection (i.e. the Bible) to
                  elucidate whether the authority and tradition of particular books exert
                  an influence on the observed intertexts: do certain books trigger a more
                  allusive or quotational intertext type? Finally, we take into account
                  the dominant topic surrounding the intertext location and examine
                  associations between the distribution of dominant topics and intertext
                  types. On the one hand, our analysis indicates that both axes (lexical
                  similarity and thematic embedding) play partially complementary roles in
                  our computational account of intertextual types. On the other hand, we
                  find that biblical books and, more strongly, dominant topics constitute
                  important factors of variation, while the authorial signal remains
                  comparatively weak.}
}

@inproceedings{chr2020:Fernandez,
  author =       {Elena Fernández Fernández and Mirco Schönfeld and Juergen Pfeffer},
  title =        {Measuring the Acceleration of the Social Construction of Time using the
                  BOE (Boletin Oficial del Estado)},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short17.pdf},
  abstract =     {The Practice of Conceptual History, by Reinhart Koselleck, explores the
                  idea that there is a direct relationship between technological
                  advancements and an acceleration in the social construction of time.
                  This paper will quantify this theory by measuring information density
                  and information variety of narratives in a BOE (Boletín Oficial del
                  Estado) dataset of thirty years (1988-2018). Using Quantitative
                  Narrative Analysis, we will define a narrative unit as a triplet of
                  Subject, Verb, Object (SVO), and we will define information density (ID)
                  as the ratio of narrative units per words per year. Afterwards, we will
                  quantify the different contexts of narratives to measure information
                  variety (IV) by constructing a network of semantic closeness from
                  trained word embeddings. This paper will present an increased IV and ID
                  over the observation time, indicating more and more facts being
                  reported. The results will show evidence of an acceleration of the
                  social construction of time.}
}

@inproceedings{chr2020:Kestemont,
  author =       {Mike Kestemont and Folgert Karsdorp},
  title =        {Estimating the Loss of Medieval Literature with an Unseen Species Model
                  from Ecodiversity},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short10.pdf},
  abstract =     {The century-long loss of documents is one of the major impediments to
                  the study of historic literature. Here we focus on Middle Dutch
                  chivalric epics (ca. 1200-1450), a genre for which little archival
                  records exist that shed light on the survival rates of works and
                  documents. We cast the quantitative estimation of these survival rates
                  as a variant of the unseen species problem from ecodiversity. We apply
                  an established non-parametric method ( Chao1 ) and compare it to a
                  number of common alternatives on simulated data. Finally, we discuss the
                  implications of our results for conventional philology: our numbers
                  suggest that the losses sustained on the level of works may be more
                  dramatic than previously imagined, whereas those at the document-level
                  align surprisingly well with existing estimates in book history,
                  although these were based on completely different data sources.}
}

@inproceedings{chr2020:Akiki,
  author =       {Christopher Akiki and Manuel Burghardt},
  title =        {Toward a Musical Sentiment (MuSe) Dataset for Affective Distant Hearing},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short26.pdf},
  abstract =     {In this short paper we present work in progress that tries to leverage
                  crowdsourced music metadata and crowdsourced affective word norms to
                  create a comprehensive dataset of music emotions, which can be used for
                  sentiment analyses in the music domain. We combine a mixture of
                  different data sources to create a new dataset of 90,408 songs with
                  their associated embeddings in Russell's model of affect, with the
                  dimensions valence , dominance and arousal . In addition, we provide a
                  Spotify ID for the songs, which can be used to add more metadata to the
                  dataset via the Spotify API.}
}

@inproceedings{chr2020:Pianzola,
  author =       {Federico Pianzola and Alberto Acerbi and Simone Rebora},
  title =        {Cultural Accumulation and Improvement in Online Fan Fiction},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short8.pdf},
  abstract =     {We analyse stories in Harry Potter fan fiction published on Archive of
                  Our Own (AO3), using concepts from cultural evolution. In particular, we
                  focus on cumulative cultural evolution, that is, the idea that cultural
                  systems improve with time, drawing on previous innovations. In this
                  study we examine two features of cumulative culture: accumulation and
                  improvement. First, we show that stories in Harry Potter's fan fiction
                  accumulate cultural traits---unique tags, in our analysis---through
                  time, both globally and at the level of single stories. Second, more
                  recent stories are also liked more by readers than earlier stories. Our
                  research illustrates the potential of the combination of cultural
                  evolution theory and digital literary studies, and it paves the way for
                  the study of the effects of online digital media on cultural
                  cumulation.}
}

@inproceedings{chr2020:Ryan,
  author =       {Yann Ryan and Sebastian Ahnert and Ruth Ahnert},
  title =        {Networking Archives: Quantitative History and the Contingent Archive},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short21.pdf},
  abstract =     {Recent years have seen a growth in the use of network analysis on large
                  datasets of correspondence, but studies of the epistemological basis for
                  findings have not seen a commensurate increase. The latter are important
                  because although large, these datasets can only ever represent a
                  fraction of the total available correspondence, and most historically
                  contingent letter archives have significant amounts of missing or
                  uncertain data or records. This paper outlines three approaches to the
                  study of missing network data: first, we suggest some strategies for
                  dealing with missing data, beginning with understanding in detail the
                  type and extent of missing data, second, we outline a method for
                  understanding the effect that missing data has specifically on
                  historical letter archives, which compares rank correlations of metrics
                  between the full network and progressively smaller random sub-samples.
                  The experiments show that the most basic metric of network structure,
                  degree, is remarkably robust to random letter removal even when large
                  samples of letters have been removed. Last, the paper argues that the
                  combinatory effect of joined-up letter networks can be used to further
                  the understanding of the structure of seventeenth-century letter
                  networks and intellectual exchange.}
}

@inproceedings{chr2020:Veleski,
  author =       {Stefan Veleski},
  title =        {Weak Negative Correlation between the Present Day Popularity and the
                  Mean Emotional Valence of Late Victorian Novels},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long44.pdf},
  abstract =     {Despite the recent upswing of computational research on Victorian
                  novels, it has largely overlooked insight from cultural evolution and
                  the cognitive sciences. This study aims to contribute to this incipient
                  scholarship by testing the hypothesis that novels containing content
                  with a lower mean emotional valence are more likely to trigger
                  recommendation-based transmission chains, and as a result tend to have
                  greater cultural longevity. This study performs a correlation analysis
                  between the mean sentiment and the contemporary popularity (using the
                  number of user ratings from Goodreads) of a selection of late Victorian
                  novels published in the United Kingdom between 1891 and 1901, taken from
                  Project Gutenberg (n=846). Moreover, the study looks into the
                  implications of this correlation for the differences between novels that
                  were bestsellers at the time of publication and those that can be
                  considered canonical today (that have recently had Broadview, Oxford
                  University, or Penguin Press editions). The results show a weak negative
                  correlation between the present day popularity and the mean emotional
                  valence of the novels, which nevertheless holds true for both the
                  bestselling and canonical novels. Moreover, canonical novels tend to
                  have a lower mean emotional valence than the bestsellers.}
}

@inproceedings{chr2020:Bleeker,
  author =       {Elli Bleeker and Bram Buitendijk and Ronald Haentjens Dekker},
  title =        {Between Flexibility and Universality: Combining TAGML and XML to Enhance
                  the Modeling of Cultural Heritage Text},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short39.pdf},
  abstract =     {This short paper first presents a conceptual workflow of a digital
                  scholarly editor, and then illustrates how the smaller components of the
                  workflow can be supported and advanced by technology. The focus of the
                  paper is on the need to encode a historical text from multiple,
                  co-existing research perspectives. Step by step, we show how this need
                  translates to a computational pipeline, and how this pipeline can be
                  implemented. The case study constitutes the transformation of a TAGML
                  document containing multiple concurrent hierarchies into an XML document
                  with one single leading hierarchy. We argue that this data
                  transformation requires input from the editor who is thus actively
                  involved in the process of text modeling.}
}

@inproceedings{chr2020:Lahti,
  author =       {Leo Lahti and Eetu Mäkelä and Mikko Tolonen},
  title =        {Quantifying Bias and Uncertainty in Historical Data Collections with
                  Probabilistic Programming},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short46.pdf},
  abstract =     {The enhanced access to ever-expanding digital data collections and open
                  computational methods have led to the emergence of new research lines
                  within the humanities and social sciences, bringing in new quantitative
                  evidence and insights. Any data interpretation depends critically on
                  understanding of the scope and limitations in data collection, as well
                  as on reliable downstream analysis. Quantitative analysis can complement
                  qualitative research by providing access to overlooked information that
                  is accessible only through systematic discovery and analysis of latent
                  patterns underlying the available data collections. Probabilistic
                  programming is an expanding paradigm in machine learning that provides
                  new statistical tools for intuitive interpretation of complex data sets.
                  This new paradigm stems from Bayesian analysis and emphasizes explicit
                  modeling of the data generating processes and associated uncertainties.
                  Despite its remarkable application potential, probabilistic programming
                  has so far received little attention in computational humanities. We use
                  a brief case study in computational history to demonstrate how
                  probabilistic programming can be incorporated in reproducible data
                  science workflows in order to detect and quantify bias in a widely
                  studied historical text collection, the Eighteenth Century Collections
                  Online.}
}

@inproceedings{chr2020:Pedrazzini,
  author =       {Nilo Pedrazzini},
  title =        {Exploiting Cross-Dialectal Gold Syntax for Low-Resource Historical
                  Languages: Towards a Generic Parser for Pre-Modern Slavic},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short48.pdf},
  abstract =     {This paper explores the possibility of improving the performance of
                  specialized parsers for pre-modern Slavic by training them on data from
                  different related varieties. Because of their linguistic heterogeneity,
                  pre-modern Slavic varieties are treated as low-resource historical
                  languages, whereby cross-dialectal treebank data may be exploited to
                  overcome data scarcity and attempt the training of a variety-agnostic
                  parser. Previous experiments on early Slavic dependency parsing are
                  discussed, particularly with regard to their ability to tackle different
                  orthographic, regional and stylistic features. A generic pre-modern
                  Slavic parser and two specialized parsers -- one for East Slavic and one
                  for South Slavic -- are trained using jPTDP nguyen , a neural network
                  model for joint part-of-speech (POS) tagging and dependency parsing
                  which had shown promising results on a number of Universal Dependency
                  (UD) treebanks, including Old Church Slavonic (OCS). With these
                  experiments, a new state of the art is obtained for both OCS (83.79 \%
                  unlabelled attachment score (UAS) and 78.43 \% labelled attachment score
                  (LAS)) and Old East Slavic (OES) (85.7 \% UAS and 80.16 \% LAS).}
}

@inproceedings{chr2020:Offert,
  author =       {Fabian Offert and Peter Bell},
  title =        {Generative Digital Humanities},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short23.pdf},
  abstract =     {While generative machine learning has recently attracted a significant
                  amount of attention in the computer science community, its potential for
                  the digital humanities has so far not been fully evaluated. In this
                  paper, we examine generative adversarial networks, a state-of-the art
                  generative machine learning technique. We argue that GANs can be
                  particularly useful in digital art history, where they can be employed
                  to facilitate the exploration of the semantic structure of large image
                  corpora. Moreover, we posit that the foundational statistical
                  distinction between discriminative and generative approaches offers an
                  alternative critical perspective on machine learning in the digital
                  humanities context. If ``all models are wrong, some are useful'', as the
                  often-cited passage reads, we argue that, in case of the digital
                  humanities, the most useful-wrong models are generative.}
}

@inproceedings{chr2020:Fonteyn,
  author =       {Lauren Fonteyn},
  title =        {What about Grammar? Using BERT Embeddings to Explore Functional-Semantic
                  Shifts of Semi-Lexical and Grammatical Constructions},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short15.pdf},
  abstract =     {The aim of this short paper is to extend the application of
                  embedding-based methodologies beyond the realm of lexical semantic
                  change. It focuses on the use of unsupervised BERT-embeddings and
                  uncertainty measures (Classification Entropy), and assesses whether (and
                  how) they can be used to (semi-)automatically flag possible
                  functional-semantic changes in the use of the construction [ BE about ]
                  in the Corpus of Historical American English (COHA).}
}

@inproceedings{chr2020:Nielbo,
  author =       {Kristoffer Nielbo and Peter Vahlstrup and Anja Bechmann and Jianbo Gao},
  title =        {Trend Reservoir Detection: Minimal Persistence and Resonant Behavior of
                  Trends in Social Media},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short3.pdf},
  abstract =     {Sociocultural trends from social media platforms such as Twitter or
                  Instagram have become an important part of knowledge discovery. The
                  `trend' construct is however ambiguous and its estimation from
                  unstructured sociocultural data complicated by several methodological
                  issues. This paper presents an approach to trend estimation that
                  combines domain knowledge of social media with advances in information
                  theory and dynamical systems. In particular, we show how trend
                  reservoirs (i.e., signals that display trend potential) can be
                  identified by their relationship between novel and resonant behavior,
                  and their minimal persistence.This approach contrasts with trend
                  estimation that relies on linear or polynomial techniques to study
                  point-like novelty behavior in social media, and it completes approaches
                  that rely on smooth functions of time.}
}

@inproceedings{chr2020:Opitz,
  author =       {Juri Opitz},
  title =        {Automatic Creation of a Large-Scale Tempo-Spatial and Semantic Medieval
                  European Information System},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long12.pdf},
  abstract =     {In this paper, we automatically create a large tempo-spatial and
                  semantic medieval information system , based on the Regesta Imperii
                  corpus, which contains abstracts of charters issued by medieval European
                  rulers. In order to build this system, we conduct the following two
                  steps: (i) place prediction : we design a bootstrapping method that
                  jointly resolves place names of the locations where a charter was
                  created together with place names mentioned in the charter texts. (ii)
                  semantic linking : we detect places, entities and their interactions
                  with dependency parsing and named entity recognition and aggregate this
                  information together with the place predictions in a single resource.
                  The final resource spans over almost 1000 years of European medieval
                  history. It contains approximately 180,000 place predictions for charter
                  origin places and place predictions for more than one million medieval
                  entities mentioned inside the charter texts, together with their
                  relations. All code is available online under public license:
                  https://github.com/flipz357/regesta-imperii-to-semgis .}
}

@inproceedings{chr2020:Enderle,
  author =       {Jonathan Scott Enderle},
  title =        {Toward a Thermodynamics of Meaning},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short40.pdf},
  abstract =     {As language models such as GPT-3 become increasingly successful at
                  generating realistic text, questions about what purely text-based
                  modeling can learn about the world have become more urgent. Is text
                  purely syntactic, as skeptics argue? Or does it in fact contain some
                  semantic information that a sufficiently sophisticated language model
                  could use to learn about the world without any additional inputs? This
                  paper describes a new model that suggests some qualified answers to
                  those questions. By theorizing the relationship between text and the
                  world it describes as an equilibrium relationship between a
                  thermodynamic system and a much larger reservoir, this paper argues that
                  even very simple language models do learn structural facts about the
                  world, while also proposing relatively precise limits on the nature and
                  extent of those facts. This perspective promises not only to answer
                  questions about what language models actually learn, but also to explain
                  the consistent and surprising success of cooccurrence prediction as a
                  meaning-making strategy in AI.}
}

@inproceedings{chr2020:Stine,
  author =       {Zachary Stine and James Deitrick and Nitin Agarwal},
  title =        {Comparative Religion, Topic Models, and Conceptualization: Towards the
                  Characterization of Structural Relationships between Online Religious
                  Discourses},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long47.pdf},
  abstract =     {The similarity between the lexicons of different religious discourses
                  does not necessarily reflect the similarity between the ways of
                  understanding the world inherent in their discourses. Drawing on
                  scholarship from comparative religion that distinguishes between
                  surface-level, lexical distinctions and deeper grammatical and
                  structural distinctions between two religious traditions, we present a
                  computational approach to assessing the structural similarity between
                  religious discourses irrespective of their lexical differences. We argue
                  that unsupervised machine learning models trained on different
                  discourses can be indirectly compared by how consistently they organize
                  information as an operationlization of structural similarity. This
                  consistency can be quantified as the mutual information between the
                  models' clusterings of a designated set of comparison data. We present
                  our approach through a case study comparing discussions from Reddit
                  concerning Buddhism and Christianity.}
}

@inproceedings{chr2020:Koolen,
  author =       {Marijn Koolen and Peter Boot and Joris van Zundert},
  title =        {Online Book Reviews and the Computational Modelling of Reading Impact},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long13.pdf},
  abstract =     {In online book reviews readers often describe their reading experience
                  and the impression that a book left. The great volume of online reviews
                  makes these reviews a great source for investigating the impact books
                  have on readers. % Recently, a reading impact model was introduced that
                  can be used to automatically identify expressions of reading impact in
                  Dutch reviews and that is able categorise them according to emotional
                  impact, aesthetic or narrative feeling, or feelings of reflection. %
                  This paper provides an analysis of the characteristics of the book
                  review domain that affect how this computational model identifies
                  impact. We look at features like the length of reviews, the nature of
                  the website on which the review was published, the genre of book and the
                  characteristics of the reviewer. % The findings in this paper provide
                  insight in how different selection criteria for reviews can be used to
                  study various aspects of reading impact.}
}

@inproceedings{chr2020:Ballance,
  author =       {Joshua Ballance},
  title =        {Pitch-Class Distributions in the Music of Anton Webern},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/short5.pdf},
  abstract =     {This paper uses pitch-class distributions as an analytical tool with
                  which to consider chromaticism in Webern's 31 published works. Previous
                  research shows that tonal music typically has a varied distribution;
                  intuition suggests that the distributions of dodecaphonic music will be
                  much more even. The range of values in the distribution is used as an
                  indication of variety, and Variability-Based Neighbour Clustering is
                  used to group the works. The results of the analysis draw attention to
                  Op. 10: three movements have idiosyncratic distributions, and the
                  highest-level partition of the corpus is within Op. 10. Expectations
                  regarding distributions for dodecaphonic music are confirmed, but the
                  tonal music diverges from typical tonal distributions.}
}

@inproceedings{chr2020:Bryan,
  author =       {Maximilian Bryan and Manuel Burghardt and Johannes Molz},
  title =        {A Computational Expedition into the Undiscovered Country - Evaluating
                  Neural Networks for the Identification of Hamlet Text Reuse},
  booktitle =    {Proceedings of the Workshop on Computational Humanities Research (CHR
                  2020)},
  date =         {18--20 November 2020},
  place =        {Amsterdam, The Netherlands},
  publisher =    {CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073)},
  volume =       {Vol-2723},
  url =          {http://ceur-ws.org/Vol-2723/long22.pdf},
  abstract =     {In this article, we describe a two-step processing pipeline for
                  identifying text reuse of Shakespeare's Hamlet in a corpus of postmodern
                  fiction by comparing n-grams from both sources. A key feature of our
                  approach lies in a pre-filtering step, in which we select target
                  sentences in the fiction corpus that are potential candidates for Hamlet
                  text reuse. Without pre-filtering, the amount of text reuse pairs (that
                  are no actual quotes) would be extremely high. In a second filtering
                  step, we compare potential text reuse pairs by their vector
                  representation using a neural network trained in an unsupervised manner.
                  We found that using the vector similarity produces a problematic amount
                  of false positives. The created vector representations are created using
                  an unsupervised training approach, resulting in similarity aspects that
                  are unfavorable for our use case.}
}
